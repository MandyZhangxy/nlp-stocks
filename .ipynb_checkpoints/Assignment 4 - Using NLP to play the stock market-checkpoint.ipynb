{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Using NLP to play the stock market\n",
    "\n",
    "In this assignment, we'll use everything we've learned to analyze corporate news and pick stocks. Be aware that in this assignment, we're trying to beat the benchmark of random chance (aka better than 50%).\n",
    "\n",
    "This assignment will involve building three models:\n",
    "\n",
    "**1. An RNN based on word inputs**\n",
    "\n",
    "**2. A CNN based on character inputs**\n",
    "\n",
    "**3. A neural net architecture that merges the previous two models**\n",
    "\n",
    "You will apply these models to predicting whether a stock return will be positive or negative in the same day of a news publication.\n",
    "\n",
    "## Your X - Reuters news data\n",
    "\n",
    "Reuters is a news outlet that reports on corporations, among many other things. Stored in the `news_reuters.csv` file is news data listed in columns. The corresponding columns are the `ticker`, `name of company`, `date of publication`, `headline`, `first sentence`, and `news category`.\n",
    "\n",
    "In this assignment it is up to you to decide how to clean this dataset. For instance, many of the first sentences contain a location name showing where the reporting is done. This is largely irrevant information and will probably just make your data noisier. You can also choose to subset on a certain news category, which might enhance your model performance and also limit the size of your data.\n",
    "\n",
    "## Your Y - Stock information from Yahoo! Finance\n",
    "\n",
    "Trading data from Yahoo! Finance was collected and then normalized using the [S&P 500](https://en.wikipedia.org/wiki/S%26P_500_Index). This is stored in the `stockReturns.json` file. \n",
    "\n",
    "In our dataset, the ticker for the S&P is `^GSPC`. Each ticker is compared the S&P and then judged on whether it is outperforming (positive value) or under-performing (negative value) the S&P. Each value is reported on a daily interval from 2004 to now.\n",
    "\n",
    "Below is a diagram of the data in the json file. Note there are three types of data: short: 1 day return, mid: 7 day return, long 28 day return.\n",
    "\n",
    "```\n",
    "          term (short/mid/long)\n",
    "         /         |         \\\n",
    "   ticker A   ticker B   ticker C\n",
    "      /   \\      /   \\      /   \\\n",
    "  date1 date2 date1 date2 date1 date2\n",
    "```\n",
    "\n",
    "You will need to pick a length of time to focus on (day, week, month). You are welcome to train models on each dataset as well.  \n",
    "\n",
    "Transform the return data such that the outcome will be binary:\n",
    "\n",
    "```\n",
    "label[y < 0] = 0\n",
    "label[y >= 0] = 1\n",
    "```\n",
    "\n",
    "Finally, this data needs needs to be joined on the date and ticker - For each date of news publication, we want to join the corresponding corporation's news on its return information. We make the assumption that the day's return will reflect the sentiment of the news, regardless of timing.\n",
    "\n",
    "\n",
    "# Your models - RNN, CNN, and RNN+CNN\n",
    "\n",
    "For your RNN model, it needs to be based on word inputs, embedding the word inputs, encoding them with an RNN layer, and finally a decoding step (such as softmax or some other choice).\n",
    "\n",
    "Your CNN model will be based on characters. For reference on how to do this, look at the CNN class demonstration in the course repository.\n",
    "\n",
    "Finally you will combine the architecture for both of these models, either [merging](https://github.com/ShadyF/cnn-rnn-classifier) using the [Functional API](https://keras.io/getting-started/functional-api-guide/) or [stacking](http://www.aclweb.org/anthology/S17-2134). See the links for reference.\n",
    "\n",
    "For each of these models, you will need to:\n",
    "1. Create a train and test set, retaining the same test set for every model\n",
    "2. Show the architecture for each model, printing it in your python notebook\n",
    "2. Report the peformance according to some metric\n",
    "3. Compare the performance of all of these models in a table (precision and recall)\n",
    "4. Look at your labeling and print out the underlying data compared to the labels - for each model print out 2-3 examples of a good classification and a bad classification. Make an assertion why your model does well or poorly on those outputs.\n",
    "5. For each model, calculate the return from the three most probable positive stock returns. Compare it to the actual return. Print this information in a table.\n",
    "\n",
    "### Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "import numpy\n",
    "import keras \n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Reshape, Conv2D, GlobalMaxPooling2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Activation, Conv1D, MaxPooling1D, Embedding, Flatten\n",
    "from keras import optimizers\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection u'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/jahuang/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK model data (you need to do this once)\n",
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "with open('stockReturns.json', 'r') as f:\n",
    "    stockReturns = json.load(f)\n",
    "news_df = pd.read_csv('news_reuters.csv', header = None, \n",
    "                      names= ['sticker', 'company', 'publication_date', 'headline', 'first_sentence', 'news_category'],\n",
    "                     encoding = 'utf-8');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join the Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join the returns and the news\n",
    "results = []\n",
    "short_gains = stockReturns['short']\n",
    "for index, row in news_df.iterrows():\n",
    "    sticker_name = row['sticker']\n",
    "    publication_date = str(row['publication_date'])\n",
    "    try:\n",
    "        gain = short_gains[sticker_name][publication_date]\n",
    "        if (gain > 0):\n",
    "            results.append(1)\n",
    "        else:\n",
    "            results.append(0)\n",
    "    except:\n",
    "        results.append(-1)\n",
    "\n",
    "news_df['outcome'] = pd.Series(results)\n",
    "news_outcome = news_df[news_df['outcome'] != -1]\n",
    "news_outcome = news_outcome.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the news content into tokens and indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 8000\n",
    "sentence_size = 120\n",
    "# Tokenize Words\n",
    "tokenized_sentences1 = [nltk.word_tokenize(sent) for sent in news_outcome['headline']]\n",
    "tokenized_sentences2 = [nltk.word_tokenize(sent) for sent in news_outcome['first_sentence']]\n",
    "tokenized_sentences = tokenized_sentences1 + tokenized_sentences2\n",
    "words = []\n",
    "for tokens in tokenized_sentences: \n",
    "    for token in tokens:\n",
    "        # remove numbers \n",
    "        if re.search('\\d', token) == None:\n",
    "            words.append(token)\n",
    "word_freq = nltk.FreqDist(words)\n",
    "vocab = word_freq.most_common(vocab_size - 1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "X = []\n",
    "for i in range(0, len(tokenized_sentences1)):\n",
    "    row = []\n",
    "    for token in tokenized_sentences1[i]:\n",
    "        index = word_to_index.get(token)\n",
    "        if index != None: \n",
    "            row.append(index)\n",
    "    for token in tokenized_sentences2[i]:\n",
    "        index = word_to_index.get(token)\n",
    "        if index != None:\n",
    "            row.append(index)\n",
    "    X.append(row)\n",
    "    \n",
    "X = sequence.pad_sequences(X, maxlen=sentence_size)\n",
    "y = news_outcome['outcome']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_51 (Embedding)     (None, 120, 32)           256000    \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 309,301\n",
      "Trainable params: 309,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 24722 samples, validate on 6181 samples\n",
      "Epoch 1/5\n",
      "24722/24722 [==============================] - 179s 7ms/step - loss: 0.6892 - acc: 0.5327 - val_loss: 0.6834 - val_acc: 0.5551\n",
      "Epoch 2/5\n",
      "24722/24722 [==============================] - 127s 5ms/step - loss: 0.6163 - acc: 0.6622 - val_loss: 0.6734 - val_acc: 0.5968\n",
      "Epoch 3/5\n",
      "24722/24722 [==============================] - 119s 5ms/step - loss: 0.5146 - acc: 0.7450 - val_loss: 0.6875 - val_acc: 0.6148\n",
      "Epoch 4/5\n",
      "24722/24722 [==============================] - 118s 5ms/step - loss: 0.4357 - acc: 0.7949 - val_loss: 0.7174 - val_acc: 0.6387\n",
      "Epoch 5/5\n",
      "24722/24722 [==============================] - 118s 5ms/step - loss: 0.3701 - acc: 0.8324 - val_loss: 0.8187 - val_acc: 0.6489\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x139e09c10>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rnn():\n",
    "    embedding_vecor_length = 32\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_vecor_length, input_length=len(X_train[0])))\n",
    "    model.add(LSTM(100))\n",
    "    # model.add(Dense(10, activation='sigmoid'))\n",
    "    # model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "rnn_model = rnn()\n",
    "rnn_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.89%\n"
     ]
    }
   ],
   "source": [
    "scores = rnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dimension = 100\n",
    "def cnn():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Embedding(input_dim = vocab_size, output_dim = embedding_dimension, input_length = sentence_size))\n",
    "    model.add(Reshape((sentence_size, embedding_dimension, 1), input_shape = (sentence_size, embedding_dimension)))\n",
    "    model.add(Conv2D(filters = 50, kernel_size = (5, embedding_dimension), strides = (1,1), padding = 'valid'))\n",
    "    model.add(GlobalMaxPooling2D())\n",
    "\n",
    "    model.add(Dense(20))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam , metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_52 (Embedding)     (None, 120, 100)          800000    \n",
      "_________________________________________________________________\n",
      "reshape_27 (Reshape)         (None, 120, 100, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 116, 1, 50)        25050     \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_17 (Glo (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 1)                 11        \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 826,291\n",
      "Trainable params: 826,291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 24722 samples, validate on 6181 samples\n",
      "Epoch 1/10\n",
      "24722/24722 [==============================] - 34s 1ms/step - loss: 0.6929 - acc: 0.5084 - val_loss: 0.6928 - val_acc: 0.4999\n",
      "Epoch 2/10\n",
      "24722/24722 [==============================] - 33s 1ms/step - loss: 0.6835 - acc: 0.5510 - val_loss: 0.6730 - val_acc: 0.5904\n",
      "Epoch 3/10\n",
      "24722/24722 [==============================] - 33s 1ms/step - loss: 0.6085 - acc: 0.6827 - val_loss: 0.6426 - val_acc: 0.6352\n",
      "Epoch 4/10\n",
      "24722/24722 [==============================] - 32s 1ms/step - loss: 0.4613 - acc: 0.8021 - val_loss: 0.6462 - val_acc: 0.6654\n",
      "Epoch 5/10\n",
      "24722/24722 [==============================] - 32s 1ms/step - loss: 0.3310 - acc: 0.8763 - val_loss: 0.7013 - val_acc: 0.6819\n",
      "Epoch 6/10\n",
      "24722/24722 [==============================] - 33s 1ms/step - loss: 0.2505 - acc: 0.9111 - val_loss: 0.8014 - val_acc: 0.6819\n",
      "Epoch 7/10\n",
      "24722/24722 [==============================] - 33s 1ms/step - loss: 0.2019 - acc: 0.9283 - val_loss: 0.8966 - val_acc: 0.6824\n",
      "Epoch 8/10\n",
      "24722/24722 [==============================] - 32s 1ms/step - loss: 0.1671 - acc: 0.9388 - val_loss: 1.1020 - val_acc: 0.6834\n",
      "Epoch 9/10\n",
      "24722/24722 [==============================] - 32s 1ms/step - loss: 0.1478 - acc: 0.9413 - val_loss: 1.1529 - val_acc: 0.6869\n",
      "Epoch 10/10\n",
      "24722/24722 [==============================] - 32s 1ms/step - loss: 0.1274 - acc: 0.9442 - val_loss: 1.2896 - val_acc: 0.6900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1221dbbd0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model = cnn()\n",
    "cnn_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.00%\n"
     ]
    }
   ],
   "source": [
    "cnn_scores = cnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (cnn_scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: RNN+CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Branch Architecture ------------------------------------------------------\n",
      "------------------------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_53 (Embedding)     (None, 120, 100)          800000    \n",
      "_________________________________________________________________\n",
      "reshape_28 (Reshape)         (None, 120, 100, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 116, 1, 50)        25050     \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_18 (Glo (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 826,070\n",
      "Trainable params: 826,070\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "RNN Branch Architecture ------------------------------------------------------ \n",
      "------------------------------------------------------------------------------ \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_54 (Embedding)     (None, 120, 32)           256000    \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 20)                2020      \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 311,220\n",
      "Trainable params: 311,220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Merged Model Architecture ---------------------------------------------------- \n",
      "------------------------------------------------------------------------------ \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_14 (Merge)             (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 10)                410       \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,137,711\n",
      "Trainable params: 1,137,711\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jahuang/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:28: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24722 samples, validate on 6181 samples\n",
      "Epoch 1/5\n",
      "24722/24722 [==============================] - 89s 4ms/step - loss: 0.6926 - acc: 0.5096 - val_loss: 0.6908 - val_acc: 0.5080\n",
      "Epoch 2/5\n",
      "24722/24722 [==============================] - 90s 4ms/step - loss: 0.6576 - acc: 0.6170 - val_loss: 0.6570 - val_acc: 0.6054\n",
      "Epoch 3/5\n",
      "24722/24722 [==============================] - 87s 4ms/step - loss: 0.5006 - acc: 0.7607 - val_loss: 0.6509 - val_acc: 0.6497\n",
      "Epoch 4/5\n",
      "24722/24722 [==============================] - 85s 3ms/step - loss: 0.3137 - acc: 0.8764 - val_loss: 0.6814 - val_acc: 0.6787\n",
      "Epoch 5/5\n",
      "24722/24722 [==============================] - 84s 3ms/step - loss: 0.2119 - acc: 0.9252 - val_loss: 0.7495 - val_acc: 0.6845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1208c6cd0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cnn_and_rnn():\n",
    "    embedding_dimension = 100\n",
    "    sentence_size = 120\n",
    "    \n",
    "    model_cnn = Sequential()\n",
    "\n",
    "    model_cnn.add(Embedding(input_dim = vocab_size, output_dim = embedding_dimension, input_length = sentence_size))\n",
    "    model_cnn.add(Reshape((sentence_size, embedding_dimension, 1), input_shape = (sentence_size, embedding_dimension)))\n",
    "    model_cnn.add(Conv2D(filters = 50, kernel_size = (5, embedding_dimension), strides = (1,1), padding = 'valid'))\n",
    "    model_cnn.add(GlobalMaxPooling2D())\n",
    "    \n",
    "    model_cnn.add(Dense(20))\n",
    "    model_cnn.add(Activation('relu'))\n",
    "    print('CNN Branch Architecture ------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------')\n",
    "    print(model_cnn.summary())\n",
    "    \n",
    "    model_rnn = Sequential()\n",
    "    model_rnn.add(Embedding(vocab_size, embedding_vecor_length, input_length=len(X_train[0])))\n",
    "    model_rnn.add(LSTM(100))\n",
    "    model_rnn.add(Dense(20))\n",
    "    model_rnn.add(Activation('relu'))\n",
    "    print('RNN Branch Architecture ------------------------------------------------------ ')\n",
    "    print('------------------------------------------------------------------------------ ')\n",
    "    print(model_rnn.summary())\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(keras.layers.Merge([model_cnn, model_rnn], mode='concat'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam , metrics=['accuracy'])\n",
    "    print('Merged Model Architecture ---------------------------------------------------- ')\n",
    "    print('------------------------------------------------------------------------------ ')   \n",
    "    print(model.summary())\n",
    "    return model\n",
    "merged_model = cnn_and_rnn()\n",
    "merged_model.fit([X_train, X_train], y_train, validation_data=([X_test, X_test], y_test), epochs=5, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.45%\n"
     ]
    }
   ],
   "source": [
    "merged_scores = merged_model.evaluate([X_test, X_test], y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (merged_scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def evaluate(test_labels, predictions): \n",
    "    precision = precision_score(test_labels, predictions, average='macro') \n",
    "    recall = recall_score(test_labels, predictions, average='macro') \n",
    "    return [precision, recall]\n",
    "    #print(\"Precision: {:.4f}, Recall: {:.4f}\".format(precision, recall))\n",
    "\n",
    "cnn_predicted = cnn_model.predict_classes(X_test)\n",
    "rnn_predicted = rnn_model.predict_classes(X_test)\n",
    "merged_predicted = merged_model.predict_classes([X_test, X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                CNN    MERGED       RNN\n",
      "metrics                                \n",
      "Precision  0.690018  0.684512  0.648926\n",
      "Recall     0.690018  0.685246  0.648997\n"
     ]
    }
   ],
   "source": [
    "cnn_metrics = evaluate(cnn_predicted, y_test)\n",
    "rnn_metrics = evaluate(rnn_predicted, y_test)\n",
    "merged_metrics = evaluate(merged_predicted, y_test)\n",
    "data = {'metrics': ['Precision', 'Recall'] ,'CNN': cnn_metrics, 'RNN': rnn_metrics, 'MERGED': merged_metrics}\n",
    "metrics_df = pd.DataFrame(data=data)\n",
    "print(metrics_df.set_index('metrics'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Good and The Bad - Classifications "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence(X_test, index):\n",
    "    word_indicies = X_test[index]\n",
    "    words = []\n",
    "    for word_index in word_indicies:\n",
    "        if (word_index != 0):\n",
    "            words.append(index_to_word[word_index])\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Classification\n",
      "\n",
      "Nvidia shows off smaller artificial intelligence computer for car Sept U.S. chipmaker Nvidia Corp showed off on Monday a smaller and more efficient artificial intelligence computer for self-driving cars saying it would power 's mapping and autonomous vehicle technology .\n",
      "\n",
      "FDA approves longer-term use of AstraZeneca blood thinner AstraZeneca Plc on Thursday said the U.S. Food and Drug Administration approved a new dose of its blood thinner Brilinta intended for longer-term use in patients with a history of heart attack or a condition known as .\n"
     ]
    }
   ],
   "source": [
    "print('Good Classification')\n",
    "print('')\n",
    "print(get_sentence(X_test, 0))\n",
    "print('')\n",
    "print(get_sentence(X_test, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad Classification\n",
      "\n",
      "MarkWest shareholder says he opposes MPLX deal Nov A shareholder of natural gas processor MarkWest Energy Partners LP John Fox said he was opposed refiner Marathon Petroleum Corp 's proposed $ billion acquisition of the company through its pipeline unit MPLX LP .\n",
      "\n",
      "Wall Street up on jobs data off Greek default NEW YORK Stocks advanced on Friday as investors off the technical default by Greece and focused instead on another strong monthly jobs report .\n"
     ]
    }
   ],
   "source": [
    "print('Bad Classification')\n",
    "print('')\n",
    "print(get_sentence(X_test, 1))\n",
    "print('')\n",
    "print(get_sentence(X_test, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN did a poor job on the second bad classification because it contained a mix of good and bad sentiment words. The first bad classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Classification\n",
      "\n",
      "Chipotle Massachusetts shut after workers fall ill Chipotle Mexican Grill Inc which is trying recover from a series of food-borne illness outbreaks temporarily shut a Massachusetts restaurant after four employees fell sick .\n",
      "\n",
      "Allergan says it stands by statements on Valeant BOSTON Oct Allergan Inc said on Tuesday that it believes there is no evidence support Valeant Pharmaceuticals and hedge fund Pershing Square Capital Management 's claims that its chief executive officer a campaign spread about Valeant .\n"
     ]
    }
   ],
   "source": [
    "print('Good Classification')\n",
    "print('')\n",
    "print(get_sentence(X_test, 6))\n",
    "print('')\n",
    "print(get_sentence(X_test, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad Classification\n",
      "\n",
      "BRIEF-Vodafone CEO urges close scrutiny of BT deal * BT deal dominant player in Britain requires scrutiny\n",
      "\n",
      "UPDATE Garden Red Lobster kids ' menus * Move comes amid calls help reduce obesity\n"
     ]
    }
   ],
   "source": [
    "print('Bad Classification')\n",
    "print('')\n",
    "print(get_sentence(X_test, 5))\n",
    "print('')\n",
    "print(get_sentence(X_test, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN seems to bias toward a couple of positive words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Classification\n",
      "\n",
      "UPDATE hands out $ million of shares in ' pay awards * bonuses for top managers scrapped ( Adds details of 's award )\n",
      "\n",
      "UPDATE 's net profit down pct on weak home front * Shares up percent vs index up pct\n"
     ]
    }
   ],
   "source": [
    "print('Good Classification')\n",
    "print('')\n",
    "print(get_sentence(X_test, 31))\n",
    "print('')\n",
    "print(get_sentence(X_test, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad Classification\n",
      "\n",
      "BRIEF-Allergan Teva entered amendment master purchase agreement * Co and pharmaceutical industries entered into an amendment dated master purchase agreement\n",
      "\n",
      "ON THE MOVE-Morgan Stanley hires three brokers from Citi Barclays Sept Morgan Stanley the world 's largest retail brokerage by its number of advisers said it hired two brokers from Citigroup Inc 's private banking unit .\n"
     ]
    }
   ],
   "source": [
    "print('Bad Classification')\n",
    "print('')\n",
    "print(get_sentence(X_test, 30))\n",
    "print('')\n",
    "print(get_sentence(X_test, 37))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These bad classifications doesn't contain any straight forward positive sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Most Probable Predicted Stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "stickers = news_outcome['company'].tolist()\n",
    "def top_three_stickers(model, data):\n",
    "    predicted = model.predict(data)\n",
    "    index1 = numpy.argmax(predicted)\n",
    "    predicted[index1] = -1;\n",
    "    index2 = numpy.argmax(predicted)\n",
    "    predicted[index2] = -1;\n",
    "    index3 = numpy.argmax(predicted)\n",
    "    top_stickers = []\n",
    "    top_stickers.append(stickers[index1])\n",
    "    top_stickers.append(stickers[index2])\n",
    "    top_stickers.append(stickers[index3])\n",
    "    return top_stickers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_top = top_three_stickers(cnn_model, X)\n",
    "rnn_top = top_three_stickers(rnn_model, X)\n",
    "cnn_and_rnn = top_three_stickers(merged_model, [X, X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            CNN                 CNN and RNN                     RNN\n",
      "0     Apple Inc          Vodafone Group Plc                  BRF SA\n",
      "1  Allergan plc  McDonald&#39;s Corporation  Intuitive Surgical Inc\n",
      "2     Amgen Inc  McDonald&#39;s Corporation  Intuitive Surgical Inc\n"
     ]
    }
   ],
   "source": [
    "top_companies = {'CNN': cnn_top, 'RNN': rnn_top, 'CNN and RNN': cnn_and_rnn}\n",
    "top_companies_df = pd.DataFrame(data=top_companies)\n",
    "print(top_companies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
